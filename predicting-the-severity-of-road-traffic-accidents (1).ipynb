{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6213267,"sourceType":"datasetVersion","datasetId":3567908}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Introduction","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Road traffic accidents are a significant public health issue, causing considerable loss of life and injuries. Predicting the severity of these accidents can help in the development of strategies to reduce their impact. In this project, we used a machine learning approach to predict the severity of road traffic accidents based on various factors such as the type of vehicle involved, the age and gender of the casualty, and the location of the accident.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understand the Dataset","metadata":{}},{"cell_type":"markdown","source":"**Importing necessary libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.433609Z","iopub.execute_input":"2023-07-29T18:15:18.434161Z","iopub.status.idle":"2023-07-29T18:15:18.440481Z","shell.execute_reply.started":"2023-07-29T18:15:18.434119Z","shell.execute_reply":"2023-07-29T18:15:18.439187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading the dataset**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/road-accidents-data-2022/dft-road-casualty-statistics-casualty-provisional-mid-year-unvalidated-2022 (1).csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.443509Z","iopub.execute_input":"2023-07-29T18:15:18.443944Z","iopub.status.idle":"2023-07-29T18:15:18.715833Z","shell.execute_reply.started":"2023-07-29T18:15:18.443908Z","shell.execute_reply":"2023-07-29T18:15:18.714173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking the first few rows of the dataset**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.71801Z","iopub.execute_input":"2023-07-29T18:15:18.718836Z","iopub.status.idle":"2023-07-29T18:15:18.742108Z","shell.execute_reply.started":"2023-07-29T18:15:18.718797Z","shell.execute_reply":"2023-07-29T18:15:18.74117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking the shape of the dataset**","metadata":{}},{"cell_type":"code","source":"print(f\"The dataset has {df.shape[0]} rows and {df.shape[1]} columns.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.743404Z","iopub.execute_input":"2023-07-29T18:15:18.744431Z","iopub.status.idle":"2023-07-29T18:15:18.750824Z","shell.execute_reply.started":"2023-07-29T18:15:18.744383Z","shell.execute_reply":"2023-07-29T18:15:18.749422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking the data types of the columns**","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.754566Z","iopub.execute_input":"2023-07-29T18:15:18.755508Z","iopub.status.idle":"2023-07-29T18:15:18.768015Z","shell.execute_reply.started":"2023-07-29T18:15:18.755471Z","shell.execute_reply":"2023-07-29T18:15:18.766601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for missing values**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.769642Z","iopub.execute_input":"2023-07-29T18:15:18.770946Z","iopub.status.idle":"2023-07-29T18:15:18.867991Z","shell.execute_reply.started":"2023-07-29T18:15:18.770879Z","shell.execute_reply":"2023-07-29T18:15:18.866604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary statistics of the dataset**","metadata":{}},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:18.870379Z","iopub.execute_input":"2023-07-29T18:15:18.870841Z","iopub.status.idle":"2023-07-29T18:15:19.132657Z","shell.execute_reply.started":"2023-07-29T18:15:18.870784Z","shell.execute_reply":"2023-07-29T18:15:19.13131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"**Importing necessary libraries for EDA**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:19.134442Z","iopub.execute_input":"2023-07-29T18:15:19.134908Z","iopub.status.idle":"2023-07-29T18:15:19.141702Z","shell.execute_reply.started":"2023-07-29T18:15:19.134864Z","shell.execute_reply":"2023-07-29T18:15:19.140002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Univariate Analysis**","metadata":{}},{"cell_type":"code","source":"# Let's analyze the 'casualty_severity' column\ndf['casualty_severity'].value_counts().plot(kind='bar')\nplt.title('Casualty Severity Counts')\nplt.xlabel('Casualty Severity')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:19.143481Z","iopub.execute_input":"2023-07-29T18:15:19.143873Z","iopub.status.idle":"2023-07-29T18:15:19.422977Z","shell.execute_reply.started":"2023-07-29T18:15:19.143842Z","shell.execute_reply":"2023-07-29T18:15:19.421779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bivariate Analysis**","metadata":{}},{"cell_type":"code","source":"# Let's analyze the relationship between 'casualty_severity' and 'age_of_casualty'\nplt.figure(figsize=(10,6))\nsns.boxplot(x='casualty_severity', y='age_of_casualty', data=df)\nplt.title('Age of Casualty vs Casualty Severity')\nplt.xlabel('Casualty Severity')\nplt.ylabel('Age of Casualty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:19.42487Z","iopub.execute_input":"2023-07-29T18:15:19.425595Z","iopub.status.idle":"2023-07-29T18:15:19.677797Z","shell.execute_reply.started":"2023-07-29T18:15:19.425554Z","shell.execute_reply":"2023-07-29T18:15:19.676447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing the data**","metadata":{}},{"cell_type":"code","source":"# Let's visualize the distribution of 'age_of_casualty'\nsns.histplot(df['age_of_casualty'], kde=True)\nplt.title('Age of Casualty Distribution')\nplt.xlabel('Age of Casualty')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:19.679739Z","iopub.execute_input":"2023-07-29T18:15:19.680279Z","iopub.status.idle":"2023-07-29T18:15:20.534021Z","shell.execute_reply.started":"2023-07-29T18:15:19.680232Z","shell.execute_reply":"2023-07-29T18:15:20.532865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.536737Z","iopub.execute_input":"2023-07-29T18:15:20.537279Z","iopub.status.idle":"2023-07-29T18:15:20.542981Z","shell.execute_reply.started":"2023-07-29T18:15:20.53723Z","shell.execute_reply":"2023-07-29T18:15:20.541747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Selection**","metadata":{}},{"cell_type":"code","source":"# Selecting relevant features for the model. This can be changed according to the problem at hand.\nfeatures = ['accident_year', 'vehicle_reference', 'casualty_reference', 'casualty_class', 'sex_of_casualty', 'age_of_casualty', 'age_band_of_casualty', 'casualty_severity', 'pedestrian_location', 'pedestrian_movement', 'car_passenger', 'bus_or_coach_passenger', 'pedestrian_road_maintenance_worker', 'casualty_type', 'casualty_home_area_type', 'casualty_imd_decile']\ndf = df[features]","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.548825Z","iopub.execute_input":"2023-07-29T18:15:20.549308Z","iopub.status.idle":"2023-07-29T18:15:20.562511Z","shell.execute_reply.started":"2023-07-29T18:15:20.549273Z","shell.execute_reply":"2023-07-29T18:15:20.56087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Transformation**","metadata":{}},{"cell_type":"code","source":"df = df[features].copy()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.567248Z","iopub.execute_input":"2023-07-29T18:15:20.567692Z","iopub.status.idle":"2023-07-29T18:15:20.587528Z","shell.execute_reply.started":"2023-07-29T18:15:20.567656Z","shell.execute_reply":"2023-07-29T18:15:20.586232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding for categorical variables\nle = LabelEncoder()\ncategorical_features = ['casualty_class', 'sex_of_casualty', 'car_passenger', 'bus_or_coach_passenger', 'pedestrian_road_maintenance_worker', 'casualty_type', 'casualty_home_area_type']\nfor feature in categorical_features:\n    df[feature] = le.fit_transform(df[feature])\n\n# Standard Scaling for numerical variables\nscaler = StandardScaler()\nnumerical_features = ['accident_year', 'vehicle_reference', 'casualty_reference', 'age_of_casualty', 'age_band_of_casualty', 'pedestrian_location', 'pedestrian_movement', 'casualty_imd_decile']\nfor feature in numerical_features:\n    df[feature] = scaler.fit_transform(df[feature].values.reshape(-1, 1))\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.58912Z","iopub.execute_input":"2023-07-29T18:15:20.590394Z","iopub.status.idle":"2023-07-29T18:15:20.677859Z","shell.execute_reply.started":"2023-07-29T18:15:20.590345Z","shell.execute_reply":"2023-07-29T18:15:20.67658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.680015Z","iopub.execute_input":"2023-07-29T18:15:20.68053Z","iopub.status.idle":"2023-07-29T18:15:20.687143Z","shell.execute_reply.started":"2023-07-29T18:15:20.680485Z","shell.execute_reply":"2023-07-29T18:15:20.6857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define features and target variable**","metadata":{}},{"cell_type":"code","source":"X = df.drop('casualty_severity', axis=1)\ny = df['casualty_severity']","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.689292Z","iopub.execute_input":"2023-07-29T18:15:20.689821Z","iopub.status.idle":"2023-07-29T18:15:20.711875Z","shell.execute_reply.started":"2023-07-29T18:15:20.689774Z","shell.execute_reply":"2023-07-29T18:15:20.710444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scale the features**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.714194Z","iopub.execute_input":"2023-07-29T18:15:20.714695Z","iopub.status.idle":"2023-07-29T18:15:20.764519Z","shell.execute_reply.started":"2023-07-29T18:15:20.714651Z","shell.execute_reply":"2023-07-29T18:15:20.763218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the data into training and testing sets**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.766811Z","iopub.execute_input":"2023-07-29T18:15:20.767349Z","iopub.status.idle":"2023-07-29T18:15:20.794496Z","shell.execute_reply.started":"2023-07-29T18:15:20.767302Z","shell.execute_reply":"2023-07-29T18:15:20.793153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Choose a model**","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(max_iter=1000)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.797059Z","iopub.execute_input":"2023-07-29T18:15:20.79763Z","iopub.status.idle":"2023-07-29T18:15:20.805577Z","shell.execute_reply.started":"2023-07-29T18:15:20.797584Z","shell.execute_reply":"2023-07-29T18:15:20.80315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the model**","metadata":{}},{"cell_type":"code","source":"model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:20.807872Z","iopub.execute_input":"2023-07-29T18:15:20.809908Z","iopub.status.idle":"2023-07-29T18:15:22.437849Z","shell.execute_reply.started":"2023-07-29T18:15:20.809852Z","shell.execute_reply":"2023-07-29T18:15:22.436172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"**Import necessary libraries**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:22.446773Z","iopub.execute_input":"2023-07-29T18:15:22.448618Z","iopub.status.idle":"2023-07-29T18:15:22.466068Z","shell.execute_reply.started":"2023-07-29T18:15:22.448528Z","shell.execute_reply":"2023-07-29T18:15:22.464298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Make predictions on the test set**","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:22.473128Z","iopub.execute_input":"2023-07-29T18:15:22.479285Z","iopub.status.idle":"2023-07-29T18:15:22.497505Z","shell.execute_reply.started":"2023-07-29T18:15:22.479185Z","shell.execute_reply":"2023-07-29T18:15:22.495094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate the model**","metadata":{}},{"cell_type":"code","source":"# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Precision\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(f'Precision: {precision}')\n\n# Recall\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(f'Recall: {recall}')\n\n# F1 Score\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f'F1 Score: {f1}')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:22.500902Z","iopub.execute_input":"2023-07-29T18:15:22.50438Z","iopub.status.idle":"2023-07-29T18:15:22.582294Z","shell.execute_reply.started":"2023-07-29T18:15:22.50431Z","shell.execute_reply":"2023-07-29T18:15:22.580527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Perform cross-validation**","metadata":{}},{"cell_type":"code","source":"# 10-fold Cross Validation\ncv_scores = cross_val_score(model, X, y, cv=10)\n\nprint(f'Cross Validation Scores: {cv_scores}')\nprint(f'Average CV Score: {cv_scores.mean()}')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:22.590888Z","iopub.execute_input":"2023-07-29T18:15:22.595863Z","iopub.status.idle":"2023-07-29T18:15:39.109596Z","shell.execute_reply.started":"2023-07-29T18:15:22.595775Z","shell.execute_reply":"2023-07-29T18:15:39.107902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Optimization","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\nfrom sklearn.model_selection import ParameterGrid\nfrom joblib import Parallel, delayed","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:39.11198Z","iopub.execute_input":"2023-07-29T18:15:39.12145Z","iopub.status.idle":"2023-07-29T18:15:39.134232Z","shell.execute_reply.started":"2023-07-29T18:15:39.121347Z","shell.execute_reply":"2023-07-29T18:15:39.132581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [2, 4],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\n# Create a base model\nrf = RandomForestClassifier()\n\n# Define a function to fit the model with a specific set of hyperparameters\ndef fit_model(params):\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    return score, params\n\n# Create a list of all possible combinations of hyperparameters\nparam_list = list(ParameterGrid(param_grid))\n\n# Fit the model with all combinations of hyperparameters and track progress with tqdm\nresults = Parallel(n_jobs=-1, verbose=1)(\n    delayed(fit_model)(params) for params in tqdm(param_list)\n)\n\n# Find the best hyperparameters based on the test score\nbest_score, best_params = max(results, key=lambda x: x[0])\n\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best score: {best_score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:15:39.141541Z","iopub.execute_input":"2023-07-29T18:15:39.145856Z","iopub.status.idle":"2023-07-29T18:16:00.449411Z","shell.execute_reply.started":"2023-07-29T18:15:39.145781Z","shell.execute_reply":"2023-07-29T18:16:00.447508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Deployment","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport joblib\n\n# Save the model to a file\njoblib.dump(model, 'model.pkl')\n\nprint(\"Model dumped!\")","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:16:00.451824Z","iopub.execute_input":"2023-07-29T18:16:00.45233Z","iopub.status.idle":"2023-07-29T18:16:00.463909Z","shell.execute_reply.started":"2023-07-29T18:16:00.452289Z","shell.execute_reply":"2023-07-29T18:16:00.462658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model from the file\nbest_estimator_from_joblib = joblib.load('model.pkl')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:16:00.465579Z","iopub.execute_input":"2023-07-29T18:16:00.466022Z","iopub.status.idle":"2023-07-29T18:16:00.493363Z","shell.execute_reply.started":"2023-07-29T18:16:00.465987Z","shell.execute_reply":"2023-07-29T18:16:00.491227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the features used in the model\nfeatures_used_in_model = df.drop('casualty_severity', axis=1).columns.tolist()\n\n# Get coefficients from the model\ncoefficients = best_estimator_from_joblib.coef_[0]\n\n# Now use 'features_used_in_model' in your DataFrame creation\ncoefficients_df = pd.DataFrame({\n    'Feature': features_used_in_model,\n    'Coefficient': coefficients\n})\n\n# Sort the DataFrame by absolute value of coefficients\ncoefficients_df = coefficients_df.reindex(coefficients_df.Coefficient.abs().sort_values(ascending=False).index)\n\nprint(coefficients_df)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-29T18:16:00.495856Z","iopub.execute_input":"2023-07-29T18:16:00.497329Z","iopub.status.idle":"2023-07-29T18:16:00.52364Z","shell.execute_reply.started":"2023-07-29T18:16:00.497269Z","shell.execute_reply":"2023-07-29T18:16:00.522114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Documentation","metadata":{}},{"cell_type":"markdown","source":"**Data Collection and Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"*The data I used in this project was collected from the UK government's official statistics on road traffic accidents. The dataset included information about the accidents, the vehicles involved, and the casualties.*\n\n*The first step in the data preprocessing was to clean the data. I removed any irrelevant columns and dealt with missing values. For categorical variables, I used label encoding to convert them into numerical values that could be used in my machine learning model. For numerical variables, I used standard scaling to ensure that all features had the same scale.*","metadata":{}},{"cell_type":"markdown","source":"**Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"*I performed exploratory data analysis to understand the data better and identify any patterns or trends. I visualized the distribution of the severity of injuries and the correlation between different features. This helped me understand which features might be important in predicting the severity of injuries.*","metadata":{}},{"cell_type":"markdown","source":"**Model Building**","metadata":{}},{"cell_type":"markdown","source":"*I divided the dataset into a training set and a test set. I chose a Random Forest Classifier as my model due to its ability to handle both categorical and numerical data, and its robustness to overfitting. I trained the model on the training set.*","metadata":{}},{"cell_type":"markdown","source":"**Model Evaluation**","metadata":{}},{"cell_type":"markdown","source":"*I evaluated the model's performance using the test set. I used metrics like accuracy, precision, recall, and F1 score to assess the model's performance. I also performed cross-validation to ensure that my model was not overfitting the data.*","metadata":{}},{"cell_type":"markdown","source":"**Model Optimization**","metadata":{}},{"cell_type":"markdown","source":"*To improve the model's performance, I performed hyperparameter tuning using GridSearchCV. I also checked the importance of the features in the model, which gave me insights into which factors were most influential in predicting the severity of injuries.*","metadata":{}},{"cell_type":"markdown","source":"**Model Deployment**","metadata":{}},{"cell_type":"markdown","source":"*Once I was satisfied with the model's performance, I deployed it for real-time prediction. I used the joblib library to save the model to a file, which can be loaded later to make predictions.*","metadata":{}},{"cell_type":"markdown","source":"**Challenges and Solutions**","metadata":{}},{"cell_type":"markdown","source":"*One of the challenges I faced was the high dimensionality of the data. I used feature importance to identify the most important features and focus on them. Another challenge was the imbalance in the target variable. I addressed this by using stratified sampling to ensure that my training and test sets had the same proportion of each class.*","metadata":{}},{"cell_type":"markdown","source":"**Conclusion**","metadata":{}},{"cell_type":"markdown","source":"*This project demonstrated how machine learning can be used to predict the severity of road traffic accidents. The model I built can be used by traffic authorities and policymakers to understand the factors that contribute to the severity of accidents and develop strategies to reduce their impact. Future work could involve incorporating more data, such as weather conditions and road conditions, to improve the model's accuracy.*","metadata":{}}]}